\chapter{Introduction}

\section{Software Repository Mining}

The field of mining software repositories (MSR) is an active and flourishing research area.
By analysing the statistical data of version control repositories, researchers can unveil interesting patterns that permeate throughout many open source projects, and formulate new empirical knowledge, either about existing software or software engineering practices in general.

For example, by studying project commits, we can identify faulty code components often subject to bug-fixing activities.
This knowledge can be further used to devise defect prediction approaches (techniques able to identify code components likely to be affected by bugs), as well as to train deep learning models capable of not only identifying, but also automatically recommending proper fixes for buggy code.

However, in order for any of these analyses to take place, an initial sample of open source projects must be taken.
This selection is usually based on general repository characteristics, which include, but aren't limited to: popularity statistics (the number of watchers, stars and forks a repository has), statistics related to project scope (the size of code, amount of commits made and code branches), or even how well maintained a project is (whether it has many open issues, whether it is updated frequently and how many direct contributors it has).
Usually metrics such as low commit and contributor counts are good indicators of "toy projects", repositories that researchers generally tend to avoid, due to a lack of useful information that can be extracted.

\section{Popular Tools and Their Limitations}

\subsection{GitHub's Official Tools}

GitHub is nowadays the primary source for code studies.
GitHub itself provides two different options for searching projects:
\begin{enumerate}
    \item GitHub's Advanced Search;
    \item The GitHub's Application Programming Interface (API).
\end{enumerate}
Unfortunately both leave much to be desired, as selecting the list of projects to analyse requires substantial effort.
While it is indeed possible to select projects using these tools, the following sections will focus on highlighting some of the biggest limitations that these tools have.

\subsubsection{Advanced Search}

Let us first dissect the online advanced search.
In it's current state, it is probably the least preferred method of sampling projects.
The search interface itself is not intuitive, requiring a certain degree of familiarity with the specific syntax that it uses for expressing parameters and values.
For example: In order to retrieve all Java repositories with 100 to 1000 stars, the user has to type:
\begin{verbatim}
stars:100..1000 language:Java
\end{verbatim}
Into the search input.
The overall amount of results obtained for broad queries is limited to only the first 1000 results.
Furthermore, the information displayed in the results themselves is also disappointing, limited to only the repository name, number of stars, main language, license, and the date of the last update.
Any further information about individual repositories can only be obtained by navigating to the main page of a repository, and even then, some information like the exact date and time of the last commit or the exact number of forks are only obtainable through unconventional
\footnote{GitHubâ€™s front end may not always display the exact numbers for stats like forks and branches.
Such stats are rounded up if their numbers are sufficiently high.
While it is still possible to find this exact data, buried in certain web-page element attributes, it is equally annoying and time-consuming to manually locate these stats for every repository.} means.
To top it all off, there is currently no way of exporting the result list into any kind of file, meaning that a great amount of labor is required to sample repositories using only the advanced search.

\subsubsection{Search API}

The Search API is arguably the superior official method of sampling GitHub repositories.
Unlike the advanced search, Representational State Transfer (REST) requests made to the search API are returned in JavaScript Object Notation (JSON), making the search results easier to process and store.
Repository-specific information provided is also an improvement, with far more descriptive data present for every search item.
The result size, although once again limited to a thousand repositories per query, is made more easily navigable with a maximum of 100 results spanning 10 pages, as opposed to the website's 10 results per 100 pages.

Although an improvement on the previously discussed advanced search, GitHub's API has a fair share of it's own distinct limitations that make it far from an optimal method for retrieving repositories.
While it does offer more detailed results, it does not include all the useful repository statistics, even some that can regularly be found on the website.
Number of contributors, commits, releases, branches and forks are all omitted from the individual search results.
It also does not offer any solution for searching broad queries.
Any API request with more than a thousand results returned has to be broken up into multiple sub-requests if the full result set is to be obtained.
Furthermore, certain key data such as repository labels and languages are not immediately provided in the initial search result set.
To acquire such data, further requests to the API have to be submitted, for each individual repository.

If it wasn't obvious by now, using GitHub's services without any automation is rather tedious.

\newpage
\subsection{Third-party Tools}

Although GitHub's tools possess significant drawbacks, there are several decent community-made alternatives for sampling project repositories.
Arguably two of the most popular are:
\begin{enumerate}
    \item GH Archive
    \item GHTorrent
\end{enumerate}

\subsubsection{GH Archive}

In Ilya Grigorik's own words: ``GH Archive is a project to record the public GitHub activity, archive it, and make it easily accessible for further analysis''. %Citation?
While this particular project is suited for observing GitHub's traffic, it's not as useful when it comes to sampling repositories.

Let's say we want to sample all the Java projects from 2012.
In order to do so, we have to look at all the activity spanning that period.
Since GH Archive keeps records of activity on an hourly basis, this means that in order to sample all repositories created in the aforementioned time frame, we have to observe all repositories linked to ``Create'' events from each hour, of each day, of each month of the year.
This translates to scanning roughly 8.6 thousand files for created events.
If that weren't enough, the amount of data you have with respect to these repositories is underwhelming, as each ``Created'' event only details the most basic of details, like the repository name, owner and master branch.
To acquire more statistics related to individual repositories, a researcher has to look at other events related to that repository that took place post-creation, and update the repository stats accordingly.
As you can already imagine, this is an overwhelming amount of data to process to acquire otherwise readily available statistics.

With all things considered, while GH Archive is a good tool for trend analysis, it falls flat when it comes to choosing repositories and retrieving their relevant statistics.
The data format provided is simply ill-suited for this particular use case.

\subsubsection{GHTorrent}

While GH Archive focuses on documenting GitHub's event stream, GHTorrent collects all information from the GitHub API. %Cite this?