\chapter{Conclusion}\label{ch:6}

The result of this 3-month long development endeavour is a functional tool, which constantly mines GitHub's API and web-pages to store information on repositories hosted on the site.
Said mined information is presented to the user through a dedicated web-page, allowing the user to both browse mined information, and export it for their own personal needs.

\section{Results}

At the time of writing, the tool has already mined more than 300000 Java repositories, with roughly 15000 repositories mined each day (assuming no server downtime on GitHub's end).
It should take roughly one year of mining to retrieve all information related to Java projects hosted on GitHub, after which other languages will be mined as well.

The querying performance is also satisfactory.
It takes on average one second to load the first search result page.
Performance dropouts may occur, but they usually occur when requesting a far off page from a lengthy result list (e.g. page 5000 from a set of 200000 results).

The time it takes to generate result list files also varies depending on the size of the result set.
Although results containing information for 1000 repositories are on average generated and sent to the client in less than 2 seconds, files containing significantly larger result lists may take longer to generate (e.g.\ generating a file containing information on 50000 repositories takes roughly 50 seconds).

\section{Future Improvements}

\subsection{Mining Parallelization}

The use of multiple parallelized miners would significantly increase the overall performance of the mining algorithm.
Parallelism allows the application to mine multiple languages at once, or even multiple intervals for a single language.
Using concurrent threads also allows the miner to create dedicated threads for mining new data, as well as threads dedicated to keeping information already stored in the database up-to-date.

Although multiple miner threads perform better than a single one, they do introduce their own issues into the mix.
Due to GitHub's host IP to token association, multiple miners on one machine would not be able to mine for long before being invalidated by GitHub's back-end.
They would not only have to use their own tokens, but also be placed on their own machines, or use IP masks and proxies to avoid detection.
Since each miner would access the same database information, we also have to employ resource locks to avoid conflicts.
All usable access tokens are kept in the same database table, and the application has to keep track of currently used tokens.

Furthermore, there are several consistency issues that would require a rework of how the app keeps track of language mining.
Crawl jobs currently only keep track of the lower mining bound.
The coverage for a language therefore starts from \textit{2008-01-01} up to the date stored in the database.
By employing parallel mining processes, the application would have to take into account when later mining jobs finish before previously stared ones.
For example let us say that the application mines three sequential intervals for a language: A, B and C\@.
If the interval B is mined before A, the current crawl job would schema would not be able to store that the B interval has been mined, without also assuming that A was mined as well.
Such assumptions should not be made lightly, as sudden internet connection or GitHub server failures can cause the process to prematurely end.
The solution is to keep track of both the interval start date, as well as the end date.
Once the application finishes crawling an interval, it checks which interval it can merge it to in the database.
This would however be more costly compared to how the application currently updates crawl coverage information.

\subsection{Result Exports}

As previously mentioned, another performance bottleneck is encountered when exporting result files.
Given that JPA repository interfaces return lists of Java objects representing the database rows, the Jackson object mappers and OpenCSV writers have to iterate over lists of results in order to write the requested information to a file.
While this is not an issue for result lists containing less than 10000 items, larger result lists may take much longer to generate files, and consequently increase server load.

There are two solutions to this issue.
The first solution involves storing files for certain queries.
Since these files can be directly sent to the user, it would no longer be necessary to access the database, and write result lists to files.
The problem with this approach is that we not only have to prepare a large amount of files for various querying combinations, but these files also have to be regularly kept up-to-date.

An alternative would be to use a library that can access the database and write query results directly to files.
This would circumvent the need for processing Java objects returned by JPA with file writers from custom libraries, and as a result improve overall performance.
However, such libraries do not currently exist, which is why one would have to be created specifically for the application.